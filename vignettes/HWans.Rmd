---
title: "homework answers"
author: "Xinyu Li"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework answers}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

This file contains all my homework answers in the class of statistcs computing. Due to the limitation of space, some parts of it have been slightly modified and omitted.

## HW0 

### Question

Use knitr to produce at least 3 examples(texts, figures, tables).

### Answers

#### Example1  

This example reads the average wage data of the United States for a period of time since 2000, the data is artificially labeled as a time series, and the method of exponential smoothing is used for fitting.

```{r,eval=TRUE}
library(finalwork)
#loading 
data(HW0data1)
attach(HW0data1)
library(stats)
#label time series
xt <- ts(t(HW0data1), start = c(2000,1), frequency = 4) 
ts.plot(xt, main="The average income in USA", ylab="income")
#fit
y1 <- HoltWinters(xt)
plot(y1)
detach(HW0data1)
```

#### Example2  

This example reads the weather data of Hefei, and then plots and displays the minimum temperature of Hefei using the functions in the ggplot2 package.

```{r,eval=TRUE}
library(finalwork)
data(HW0data2)
attach(HW0data2)
library(ggplot2)
library(lubridate)
#plot
data_time <- data.frame(day = ymd(HW0data2[,1]), lowtemp =HW0data2[,3])
ggplot(data_time, aes(x=day, y=lowtemp))+geom_line(color="blue") + xlab("")+
scale_x_date(date_breaks = "2 years")+
labs(title="The lowest temperature in Hefei")+
theme(axis.text.x=element_text(angle=30,hjust=0.8,vjust=0.8))
detach(HW0data2)
```

#### Example3  

This example models and constructs a time series of length 1000 for the following AR sequence.

$$
x_t=1.4x_{t-1}-0.6x_{t-2}+\epsilon_t,\quad \epsilon_t \    i.i.d.\sim \mathcal{N}(0,1)
$$

```{r,eval=TRUE}
library(graphics)
library(stats)
a<-1.4;b<--0.6
AR2 <- function(eps) {
  x=rep(0,length(eps))   #construct x
  x[1:2]=eps[1:2]        #initialize
  for (t in 3:length(eps)) {  #xt construct
    x[t]=a*x[t-1]+b*x[t-2]+eps[t]
  }
  return(x)
}

#set seed and generate series
set.seed(1234)
epsi <- rnorm(1000, mean = 0, sd=1)
Xt<- AR2(epsi)
plot(Xt, type="l",col="blue",xlab="time",ylab="",main=expression(X[t]))
```

## HW1  

### Question

- Use the inverse transform to replicate part of sample (replace=TRUE) in a function named my.sample.

- Exercises 3.2, 3.7, 3.9, 3.10 (pages 94-96, Statistical Computing with R).

### Answers

#### Exercise1  

Refer to the courseware example, define $x$ as sampling sample, $prob$ as sampling probability, and the default value is equal probability sampling.  

```{r,eval=TRUE}
my.sample<-function(x,size,replace=TRUE,prob=rep(1/length(x),length(x))){
  #size is the number of samples taken, and prob is the probability of sampling weight
  cp<-cumsum(prob)      #Divide the probability 1 interval from 0 to 1
  U<-runif(size)        
  r<-x[findInterval(U,cp)+1]    #Find the subscript of x corresponding to the interval of cp where u is, and then extract x
  return(r)      #Return the number of samples
}

s<-sample(c(1,2,4,10),size=10,replace=TRUE)
my.s<-my.sample(c(1,2,4,10),size=10,replace=TRUE)
print(s)
print(my.s)
```

#### Exercise 3.2

From $f(x)$, when $x>0$, we have $F(x)=1-\frac 1 2e^{-x}$, which means $X=F_X^{-1}(U)=-log(2-2U),U\in (1/2,1)$. When $x<=0$, we have $F(x)=\frac 1 2e^{x}$, which means $X=F_X^{-1}(U)=log(-2U),U\in (0,1/2]$.     
According to the algorithm, 1000 random numbers obey the standard uniform distribution, and then $X$is obtained by dividing $u>1/2$ and $u<=1/2$ using the above distribution function.
Note: In the title of the image generated by the code, $x$ represents $|x|$ because it is not good to output the absolute value sign of latex.

```{r,eval=TRUE}
library(graphics)
set.seed(2023)
n<-1000    #number of random variables
u<-runif(n)  #generate u
x<-numeric(length(u))
x[which(u<=1/2)]<-log(2*u[which(u<=1/2)])   #find u <=1/2
x[which(u>1/2)]<--log(2-2*u[which(u>1/2)]) 
#find u >1/2

#histogram plot
hist(x, prob = TRUE, main = expression(f(x)==1/2*e^{-x}),ylim=c(0,0.6))
y <- seq(-6, 6, 0.05)
lines(y, 1/2*exp(-abs(y)))
```

#### Exercise 3.7

$f(x)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}$, take the derivative and get $x_0=argmax_{x \in (0,1)}f(x)=\frac{a-1}{a+b-2}$(here we consider $a>=2$), then we choose $g(x)=1,0<x<1$ and $c=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x_0^{a-1}(1-x_0)^{b-1}$.
Algorithmï¼š   
1.generate $U\sim U(0,1)$ and $Y \sim U(0,1)$,  
2.If $U\leq \frac{f(Y)}{cg(Y)}$, accept $Y$, otherwise reject and continue.   

```{r,eval=TRUE}
library(stats)
library(graphics)
set.seed(2023)
generate_beta<-function(n,a,b){
  #a,b~Beta(a,b)
  j<-k<-0;y<-numeric(n)
  x0<-(a-1)/(a+b-2)
  c<-x0^(a-1)*(1-x0)^(b-1)
  while(k<n){
    u<-runif(1)
    j<-j+1
    x<-runif(1)
    if(x^(a-1)*(1-x)^(b-1)/c>u){  #accept x
      k<-k+1;y[k]<-x
    }
  }
  return(y)
}

x<-generate_beta(1000,3,2)
#histogram plot
hist(x, prob = TRUE, main = paste('Beta(',3,',',2,')'))
y <- seq(0, 1, 0.01)
lines(y, y^(3-1)*(1-y)^(2-1)*gamma(3+2)/gamma(3)/gamma(2))
```

#### Exercise 3.9

According to the above algorithm, $U_1,U_2,U_3$are generated and selected according to the size.

```{r,eval=TRUE}
library(stats)
library(graphics)
set.seed(2023)
n<-10000
x<-numeric(n)
for (i in 1:n){
  u<-runif(3,-1,1)   #generate u1,u2,u3
  if (abs(u[3])>=abs(u[2]) & abs(u[3])>=abs(u[1])) x[i]<-u[2]
  else x[i]<-u[3]
}
#histogram plot
hist(x, prob = TRUE, main = expression(f(x)==3/4(1-x^2)))
y <- seq(-1, 1, 0.01)
lines(y, 3/4*(1-y^2))
```

#### Exercise 3.10

Suppose the sample extracted by the algorithm is $X$, then the algorithm knows that the value of $X$is $[-1,1]$ and its distribution is symmetric (according to the conditional judgment statement when taking $X$). Then we have $P(|X| \leq x)=2P(X\leq x),x\in [0,1]$ and 

$$
\begin{aligned}\small
&P(|X|\leq x)\\
&=P(max(|U_1|,|U_2|)\leq |U_3|,|U_2|\leq x)+P(max(|U_1|,|U_2|)> |U_3|,|U_3|\leq x)   \\
&=P(|U_1|\leq|U_2|\leq |U_3|,|U_2|\leq x)+P(|U_2|\leq|U_1|\leq |U_3|,|U_2|\leq x)+P(max(|U_1|,|U_2|)> |U_3|,|U_3|\leq x)   \\
&=\int_0^xdu_2 \int_0^{u_2}2\cdot \frac 1 2du_1 \int_{u_2}^1 2\cdot\frac 1 2du_3+\int_0^xdu_2 \int_{u_2}^{1}2\cdot \frac 1 2du_1 \int_{u_1}^1 2\cdot\frac 1 2du_3 +P(|U_1|> |U_3|,|U_3|\leq x)^2  \\
&=\int_0^x du_2 [\int_0^{u_2}(1-u_2)du_1+\int_{u_2}^1(1-u_1)du_1]+\int_0^x 2 \cdot\frac 1 2(1-u_2^2)du_3  \\
&=\frac{3x-x^3}{2}
\end{aligned}
$$
where $\frac 1 2$ is $U_k$ p.d.f.. So$P(X\leq x)=\frac{3x-x^3}{4}$. This is also the distribution function corresponding to $f_e(x)$, indicating that the random sample extracted by the algorithm does satisfy the density distribution of $f_e(x)$.

## HW2

### Question

- Ex1
  - Proof that what value $\rho=l/d$ should take to minimize the asymptotic variance of $\hat \pi$ (m~B(n,p),using $\delta$ method)

  - Take three different values of $\rho$ ($0 \leq \rho \leq 1$, including $\rho_{min}$) and use Monte Carlo simulation to verify
your answer. (n = $10^6$, Number of repeated simulations K = 100)

- Ex2
  - Exercises 5.6, 5.7 (pages 149-151, Statistical Computing with R)
  
### Answers

#### Ex1  

According to the calculation of the pdf., we have $\hat p=\frac{m}{n}=\frac{2\rho}{\hat \pi}$ (note that here the definition of $\hat p$ and pdf are opposite), namely $$\hat \pi=\frac{2\rho}{\hat p}$$. According to the binomial distribution and the central limit theorem we have $\sqrt{n}(\hat p-p)\rightarrow N(0,p(1-p))$$, and then according to the delta method we have

$$
\sqrt{n}(\hat \pi-\pi) \rightarrow N(0,p(1-p)4\rho^2/p^4)
$$
where define $g(p)=2\rho/p$, take the derivation and get the above equation. Then we use the definition of $p$ to obtain $p=2\rho/ \pi$,
$$
\sqrt{n}(\hat \pi-\pi) \rightarrow N(0,\pi^2(1-p)/p),
$$

as $0 \leq \rho \leq 1$, due to $2\rho/p = \pi$, we have the asymptotic variance is minimum when $\rho=1$.

(2) Take $\rho_1=0.3,\rho_2=0.7,\rho_3=1,d=1$.

```{r,eval=TRUE}
library(stats)
set.seed(12345)
#set rho
rho1=0.3;rho2=0.7;rho3=1;d=1
#set n and K
n <- 1e6;K=100
pihat1<-numeric(K);pihat2<-numeric(K);pihat3<-numeric(K)
#MC simulation
for (k in 1:K){
  X <- runif(n,0,d/2)
  Y <- runif(n,0,pi/2)
  #calculate pihat
  pihat1[k]<-2*rho1/mean(rho1/2*sin(Y)>X)
  pihat2[k]<-2*rho2/mean(rho2/2*sin(Y)>X)
  pihat3[k]<-2*rho3/mean(rho3/2*sin(Y)>X)
}
pihat1_MC<-mean(pihat1)
pihat2_MC<-mean(pihat2)
pihat3_MC<-mean(pihat3)
var<-numeric(3)
#calculate var(pihat)
var[1]<-var(pihat1);var[2]<-var(pihat2);var[3]<-var(pihat3)
var
```

According to the calculated variance, it can be found that the variance when $\rho=1$is the smallest, which accords with the theoretical result of the first question, and it can also be seen from the first question that the asymptotic variance will decrease with the increase of $\rho$, which is also explained by the MC experiment.     

#### Exercise 5.6

$$
\begin{aligned}
Cov(e^U,e^{1-U})&=E[e^Ue^{1-U}]-E[e^U]E[e^{1-U}]=e-\int_0^1e^x\cdot1dx \int_0^1e^{1-x}\cdot1dx \\
&=e-(e-1)^2=-0.2342106   \\
Var(e^U+e^{1-U})&=Var(e^U)+Var(e^{1-U})+2Cov(e^U,e^{1-U})\\
&=-e^2+4e-3+2e-2(e-1)^2=0.01564999
\end{aligned}
$$

According to simple MC sample, we have the estimator of $\theta$ is $\hat \theta_1=1/n\sum_{i=1}^ne^{U_i}$, $U_i \quad i.i.d.~U(0,1)$, while use dual MC sample we have the estimator of $\theta$ is $\hat \theta_2=1/n\sum_{i=1}^n(e^{U_i}+e^{1-U_i})/2$, $U_i \quad i.i.d.~U(0,1)$. Use the assumption of $i.i.d.$, we have

$$
\begin{aligned}
Var(\hat \theta_1)&=\frac1 n Var(e^U)=[-1/2e^2+2e-3/2]/n=0.2420356/n  \\
Var(\hat \theta_2)&=\frac 1 {4n}Var(e^U+e^{1-U})=0.01564999/(4n)
\end{aligned}
$$

So the variance decreases by a ratio of $\frac{Var(\hat \theta_1)-Var(\hat \theta_2)}{Var(\hat \theta_1)}=98.38\%$.

#### Exercise 5.7

```{r,eval=TRUE}
library(stats)
set.seed(12345)
m <- 1e4
x <- runif(m, min=0, max=1)
#MC
theta_hat1 <- mean(exp(x))
theta_hat2<-mean((exp(x)+exp(1-x))/2)
#theta_hat
theta_hat1
theta_hat2
#var(theta_hat)
var1 <- var(exp(x))                   
var2 <- var((exp(x)+exp(1-x))/2)      
#reduction ratio
100*(var1-var2)/var1
```

## HW3

### Question

- Ex1(see figure in the pdf in qq)

- Ex2
  - Exercises 5.13, 5.14, 5.15 (pages 149-151, Statistical Computing with R) Computing with R)
  
  - Exercises 6.5, 6.A (pages 180-181, Statistical Computing with R).
  
### Answers

#### Ex1

Here $Var(\theta_I)$ misses $1/M$.
We just need to proof $\frac{1}{M} Var(\theta_I)\rightarrow Var(\hat\theta_M)=\frac{1}{M}Var(g(U))$, i.e.
$$
lim_{k \rightarrow \infty} \sum_{i=1}^k\int_{a_i}^{b_i} g_0^2(U)=\int_a^b g^2(U)
$$
where $g_0(U)$ is the step function generated by $g$ on the interval. According to the approximation theory of step function in real analysis, the above formula is valid.

#### Exercise 5.13

We choose    
$$
\begin{aligned}
f_1(x)&=\frac{1}{\sqrt{2\pi}}e^{-x^2/2},\\
f_2(x)&=\frac{1}{2}x^2e^{-x}.
\end{aligned}
$$

in factr, $f_1$ follows $N(0,1)$, $f_2$ follows $\Gamma(3,1)$, the function form is close to $g(x)$ and easy to sample.

```{r,eval=TRUE}
library(stats)
##define function
g<-function(x) 1/sqrt(2*pi)*x^2*exp(-x^2/2)*(x>1)
f1<-function(x) dnorm(x)
f2<-function(x) dgamma(x,3,1)

##sample
set.seed(123)
m <- 10000
theta.hat <- se<- numeric(2)
fg<-matrix(0,nrow=m,ncol=2)  #m*2, Column i stores the f/g value generated by fi

###use f1
x <- rnorm(m)
fg[,1]<-g(x)/f1(x)
theta.hat[1] <-mean(fg[,1])
se[1] <- sd(fg[,1])

###use f2
x <- rgamma(m,3,1)
fg[,2]<-g(x)/f2(x)
theta.hat[2] <-mean(fg[,2])
se[2] <- sd(fg[,2])

##show results
rbind(theta.hat, se)
```

According to the results, the estimated standard deviation of $f_2$is smaller. I think this result is caused by the difference in the intersection range of the support domains of the two importance functions and the support domains of $g$: For a normal distribution, $x$ is all real numbers, but the support domain of $g$ is $(1,\infty)$, so that $g(x)/f(x)$ has many zero values (more than 75\%), and other non-zero values and near-zero means cause large variance; However, the proportion of $g(x)/f(x)$ being zero for the gamma distribution is smaller (less than 25\%), so it performs better than $f_1$. The following code helps illustrate this conclusion.

```{r,eval=TRUE}
summary(fg[,1])
summary(fg[,2])
```

#### Exercise 5.14

Choose $f_2$ as an importance sampling function, the following code generates an MC estimate.

```{r,eval=TRUE}
g<-function(x) 1/sqrt(2*pi)*x^2*exp(-x^2/2)*(x>1)
f1<-function(x) dnorm(x)
f2<-function(x) dgamma(x,3,1)

##sample
set.seed(123)
m <- 10000
theta.hat<-numeric(m)
fg<-numeric(m)

###use f2
x <- rgamma(m,3,1)
fg<-g(x)/f2(x)
theta.hat <-mean(fg)

##MC estimate
theta.hat
```

#### Exercise 5.15   

We need to estimate  $\theta=\int_0^1\frac{e^{-x}}{1+x^2}dx$, and we choose $f_3(x)=e^{-x}/(1-e^{-1}),0<x<1$ as an importance sampling function. Here the inverse transformation method is used to generate a random number with a distribution of $f_3$. 
    
According to the question, the sampling interval was divided into 5 segments, i.e. $(F^{-1}((j-1)/5),F^{-1}(j/5)),j=1,\dots,5$. In the jth interval, the variable is generated based on $$5\frac{e^{-x}}{1-e^{-1}},(F^{-1}((j-1)/5)<x< F^{-1}(j/5))$$, where $F^{-1}(t)=-log(1-x(1-e^{-1}))$ is chosen as the importance sampling function. 

```{r,eval=TRUE}
library(stats)
m <- 10000
g <- function(x) exp(-x)/(1+x^2)*(x>0)*(x<1)
f <- function(x) exp(-x)/(1-exp(-1))*(x>0)*(x<1)

set.seed(123)
k<-5
n<-m/k  #Number of repetitions per interval
theta.hat <- var.hat <-numeric(k)
for(i in 1:k){
  u <- runif(n,(i-1)/5,i/5)
  x <- -log(1-(1-exp(-1))*u)  #generate x
  fg <- g(x)/k/f(x)   #gj/fj
  theta.hat[i]<-mean(fg)   #estimate
  var.hat[i]<-var(fg)
}
sum(theta.hat)
sqrt(sum(var.hat))
```

In 5.10, $\hat\theta=0.5257801,se(\hat\theta)=0.0970314$. The estimates are similar but the variance is much larger than the result of 5.13.  

#### Exercise 6.5  

Use the following equation to construct t-CI:
$$
\frac{\bar{X} -\mu}{\sqrt{S^2/n}}\sim t_{n-1}
$$
where $S$ is the sample variance.  

```{r,eval=TRUE}
library(stats)
# set parameters
N <- 10000  # number of simulation
sample_size <- 20       
cl<- 0.95  # 1-alpha

# initialize
coverage <- numeric(N)

# MC
for (i in 1:N) {
  # generate samples
  sample_data <- rchisq(sample_size, df = 2)
  
  # calculate mean and sd
  sample_mean <- mean(sample_data)
  sample_se<-sd(sample_data) / sqrt(sample_size)
  
  # t-interval
  t_value <- qt((1 - cl) / 2, lower.tail=FALSE,df = sample_size - 1)
  lower_bound <- sample_mean - t_value * sample_se
  upper_bound <- sample_mean + t_value * sample_se
  
  # test
  true_mean_covered <- (lower_bound <= 2) & (upper_bound >= 2)
  
  # reserve results
  coverage[i] <- true_mean_covered
}

# prob
coverage_probability <- mean(coverage)
coverage_probability

```

This result is similar to that in Example 6.4, where the coverage probability is approximately 0.95, but if the data is strictly followed by a normal distribution, the estimate of the mean can be closer to 0.95. This result shows that the mean estimation is closer to the confidence coefficient when the data does not follow the normal distribution, which is more robust than the variance estimation.

#### Exercise 6.A

The first type of error probability of the test is given in three cases.  

```{r,eval=TRUE}
library(stats)
set.seed(123)
##chi2
N <- 10000  
sample_size <- 30       
alpha<- 0.05  
mu0<-1
# initialize
p1 <- numeric(N)
p2 <- numeric(N)
for (j in 1:N) {
  x <- rchisq(sample_size, df = 1)
  ttest <- t.test(x,  mu = mu0)
  p1[j] <- ttest$p.value
  t_stat <- (mean(x) - mu0) / (sd(x) / sqrt(sample_size))
  p2[j]<-sum(abs(t_stat)>qt(1-alpha/2,df=sample_size-1))
}
p1.hat <- mean(p1 < alpha)
p2.hat<-mean(p2)
print(c(p1.hat, p2.hat))
```

```{r,eval=TRUE}
library(stats)
set.seed(123)
##uniform
N <- 10000  
sample_size <- 30      
alpha<- 0.05 
mu0<-1
# initialize
p1 <- numeric(N)
p2 <- numeric(N)
for (j in 1:N) {
  x <- runif(sample_size, 0,2)
  ttest <- t.test(x,  mu = mu0)
  p1[j] <- ttest$p.value
  t_stat <- (mean(x) - mu0) / (sd(x) / sqrt(sample_size))
  p2[j]<-sum(abs(t_stat)>qt(1-alpha/2,df=sample_size-1))
}
p1.hat <- mean(p1 < alpha)
p2.hat<-mean(p2)
print(c(p1.hat, p2.hat))
```

```{r,eval=TRUE}
library(stats)
set.seed(123)
##exp
N <- 10000  
sample_size <- 30       
alpha<- 0.05  
mu0<-1
# initialize
p1 <- numeric(N)
p2 <- numeric(N)
for (j in 1:N) {
  x <- rexp(sample_size, 1)
  ttest <- t.test(x,  mu = mu0)
  p1[j] <- ttest$p.value
  t_stat <- (mean(x) - mu0) / (sd(x) / sqrt(sample_size))
  p2[j]<-sum(abs(t_stat)>qt(1-alpha/2,df=sample_size-1))
}
p1.hat <- mean(p1 < alpha)
p2.hat<-mean(p2)
print(c(p1.hat, p2.hat))
```

Under the same conditions, the probability of the first type of error in uniform distribution is the closest to 0.05, followed by exponential distribution, and the probability of the first type of error in Chi-square distribution is the highest, which is close to 0.1. On the one hand, it shows the robustness of t-test mean value test, that is, it can also get better test results for non-normal distribution. On the other hand, it shows that the uniform distribution and exponential distribution are more similar to the normal distribution in the case of the question, so the test results are more inclined to the t-test test in the case of normal distribution.

## HW4

### Question

- Ex1(see figure in the pdf in qq)

- Ex2(see figure in the pdf in qq)

- Ex3
  Obtain a bootstrap t confidence interval estimate for the correlation statistic in Example 7.2 (law data in bootstrap). 
  
### Answers   

#### Ex1   

Here consider FWER calculated by definition, the number of experiments in which the adjusted p-value rejects the null hypothesis in all cases where the null hypothesis holds/the total number of experiments.
Considering that FDR is calculated according to the definition, the number of p-values that reject the null hypothesis for the adjusted p-values that hold the null hypothesis/the number of p-values that reject the null hypothesis for the adjusted p-values, and the expectation is estimated by averaging all the simulated FDR values.
Considering that TDR is calculated according to the definition, the number of p-values that reject the original hypothesis for the adjusted p-value if the opposing hypothesis is true/the number of opposing hypotheses is true, and the expectation is estimated by averaging all the simulated FDR values.

```{r,eval=TRUE}
library(stats)
set.seed(123)
m<-1000;M<-1000;alpha<-0.1  
reject1_h0<-matrix(0,nrow=M,ncol=m)
reject2_h0<-matrix(0,nrow=M,ncol=m)
result<-list()
result[[1]]<-matrix(0,nrow=M,ncol=3)  #Bonferroni
result[[2]]<-matrix(0,nrow=M,ncol=3)  #B-H
for (simu in 1:M){
  p_H0<-runif(m*0.95,0,1)
  p_Ha<-rbeta(m*0.05,0.1,1)
  p<-c(p_H0,p_Ha)
  p.adj1 <- p.adjust(p,method='bonferroni')
  p.adj2 <-p.adjust(p,method='fdr')
  
  #reject?
  reject1_h0[simu,] <- p.adj1 < alpha
  reject2_h0[simu,] <- p.adj2 < alpha
}

#FWER
fwer1<-0;fwer2<-0;fdr1<-0;fdr2<-0;tdr1<-0;tdr2<-0
for (simu in 1:M){
  fwer1<-fwer1+(sum(reject1_h0[simu,(1:m*0.95)])>0)
  fwer2<-fwer2+(sum(reject2_h0[simu,(1:m*0.95)])>0)
  fdr1<-fdr1+sum(reject1_h0[simu,(1:m*0.95)])/sum(reject1_h0[simu,])
  fdr2<-fdr2+sum(reject2_h0[simu,(1:m*0.95)])/sum(reject2_h0[simu,])
  tdr1<-tdr1+sum(reject1_h0[simu,(m*0.95+1):m])/(m*0.05)
  tdr2<-tdr2+sum(reject2_h0[simu,(m*0.95+1):m])/(m*0.05)
}

result<-matrix(0,nrow=2,ncol=3)
result[1,1]<-fwer1/M;result[2,1]<-fwer2/M 
result[1,2]<-fdr1/M;result[2,2]<-fdr2/M
result[1,3]<-tdr1/M;result[2,3]<-tdr2/M
```

summary the above results:         
```{r,eval=TRUE}
result<-as.data.frame(result)
rownames(result)<-c("Bonferroni","B-H")
colnames(result)<-c("FWER","FDR","TPR")
knitr::kable(head(result),format = "markdown")
```

#### Ex2    

```{r,eval=TRUE}
library(stats)
set.seed(123)
lambda_true<-2
sample_num<-c(5,10,20)
B<-1000
m<-1000  #simulation numbers
results<-list()
for(i in 1:length(sample_num)){  
  results[[i]]<-matrix(rep(0,m*2),nrow=m) 
  colnames(results[[i]])<-c("bias","sd")
}
for (sam in 1:length(sample_num)){   #1:3
  for (simu in 1:m){
    x<-rexp(sample_num[sam],lambda_true)  
    thetastar<- numeric(B)
    theta <- 1/mean(x)
    for(b in 1:B){
      xstar<-sample(x,replace = TRUE)
      thetastar[b]<-1/mean(xstar)
    }
    results[[sam]][simu,1]<-round(mean(thetastar)-theta,3)
    results[[sam]][simu,2]<-round(sd(thetastar),3)
  }
}
for (sam in 1:length(sample_num)){
  cat("sample number is ",sample_num[sam],",result is:","\n",sep="")
  result<-results[[sam]]
  cat("bootstrap bias mean",round(mean(result[,1]),3),"\n",sep="")
  cat("bootstrap sd mean",round(mean(result[,2]),3),"\n",sep="")
}
```

summary the above results:        
```{r,eval=TRUE}
dat<-matrix(rep(0,3*4),nrow=3)
dat<-as.data.frame(dat)
rownames(dat)<-c("n=5","n=10","n=20")
colnames(dat)<-c("bias_bs","bias_theory","se_bs","se_theory")
for (sam in 1:length(sample_num)){
  result<-results[[sam]]
  dat[sam,c(1,3)]<-c(round(mean(result[,1]),3),round(mean(result[,2]),3))   #mean
  dat[sam,c(2,4)]<-c(round(lambda_true/(sample_num[sam]-1),3),round(lambda_true*sample_num[sam]/(sample_num[sam]-1)/sqrt(sample_num[sam]-2),3))   #theoretical value
  
}
knitr::kable(head(dat),format = "markdown")
```

It can be seen from the table that with the increase of sample size, the deviation of the estimator obtained by bootstrap (average value in the sense of 1000 repetitions) and its standard deviation (average value in the sense of 1000 repetitions) are closer and closer to the theoretical value.

#### Ex3   

```{r,eval=TRUE}
#refer to the textbook
boot.t.ci <-function(x, B = 500, R = 100, level = .95, statistic){
#compute the bootstrap t CI
  x <- as.matrix(x); n <- nrow(x)
  stat <- numeric(B); se <- numeric(B)
  boot.se <- function(x, R, f) {  #x^(b) use bootstrap
    #local function to compute the bootstrap
    #estimate of standard error for statistic f(x)
    x <- as.matrix(x); m <- nrow(x)
    th <- replicate(R, expr = {
      i <- sample(1:m, size = m, replace = TRUE)
      f(x[i, ])})
    return(sd(th))
  }
  for (b in 1:B) { 
    j <- sample(1:n, size = n, replace = TRUE)
    y <- x[j, ]
    stat[b] <- statistic(y)
    se[b] <- boot.se(y, R = R, f = statistic)
  }
  stat0 <- statistic(x)
  t.stats <- (stat - stat0) / se
  se0 <- sd(stat)
  alpha <- 1 - level
  Qt <- quantile(t.stats, c(alpha/2, 1-alpha/2), type = 1)
  names(Qt) <- rev(names(Qt))
  CI <- rev(stat0 - Qt * se0)
}

```

```{r,eval=TRUE}
library(bootstrap) #for the law data
data <- cbind(law$LSAT, law$GPA)  #data
stat <- function(data) {   
  cor(data[, 1],data[, 2])}
ci <- boot.t.ci(data, statistic = stat, B=2000, R=200)
print(ci)
```

## HW5

### Question

Exercises 7.5 7.8 7.11 (pages 212-213, Statistical Computing with R).

### Answers

#### Exercise 7.5    

```{r,eval=TRUE}
library(boot) 
library(graphics)
set.seed(12345)
dat <- aircondit$hours #data
boot.mean <- function(x,i) mean(x[i])
de <- boot(data=dat,statistic=boot.mean, R = 2000)
ci <- boot.ci(de,type=c("norm","basic","perc","bca"))
print(ci)
hist(de$t, prob = TRUE,main="Bootstrap replicates")
```

The 95% bootstrap confidence interval for the mean time between failures calculated by the standard normal method is (34.4,181.4), the Basic Law is (24.3,170.1), the percentile method is (46.1,191.9), and the BCa method is (55.6,224.7), which are different from each other and the interval length is different.

It can be seen from the histogram of each repeat that the density is not approximately normal, so the normal and percentile intervals are different. The presumed reason is that the sample is too small and CLT cannot give a good approximation here. The BCa interval is a percentile type interval, but it adjusts for skewness and bias, so the BCa interval is significantly different from the other intervals.

#### Exercise 7.8   

```{r,eval=TRUE}
library(bootstrap)
set.seed(12345)
dat <- scor;n<-nrow(dat)#data
#jackknife estimate bias
#theta_hat
cov.hat <- cov(scor)
evs.hat <- eigen(cov.hat)$values
theta.hat<-max(evs.hat)/sum(evs.hat)
#theta.jack
theta.jack <- numeric(n)
for(i in 1:n){
  cov.jack <- cov(scor[-i,])
  evs.jack <- eigen(cov.jack)$values
  theta.jack[i] <- max(evs.jack)/sum(evs.jack)
}
#bias&se
bias.jack <- (n-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((n-1)*mean((theta.jack-theta.hat)^2))
round(c(original=theta.hat,bias.jack=bias.jack,
se.jack=se.jack),3)
```

#### Exercise 7.11   

```{r,eval=TRUE}
library(DAAG)
library(stats)
set.seed(12345)
attach(ironslag)
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n*(n-1)/2)
# for n-fold cross validation
# fit models on leave-two-out samples
indexs<-combn(1:n,2)
for (ind in 1:ncol(indexs)) {
  y <- magnetic[-indexs[,ind]]
  x <- chemical[-indexs[,ind]]
  
  J1 <- lm(y ~ x)
  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[indexs[,ind]]
  e1[ind] <- sum((magnetic[indexs[,ind]] - yhat1)^2)
  
  J2 <- lm(y ~ x + I(x^2))
  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[indexs[,ind]] +J2$coef[3] * chemical[indexs[,ind]]^2
  e2[ind] <- sum((magnetic[indexs[,ind]] - yhat2)^2)
  
  J3 <- lm(log(y) ~ x)
  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[indexs[,ind]]
  yhat3 <- exp(logyhat3)
  e3[ind] <- sum((magnetic[indexs[,ind]] - yhat3)^2)
  
  J4 <- lm(log(y) ~ log(x))
  logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[indexs[,ind]])
  yhat4 <- exp(logyhat4)
  e4[ind] <- sum((magnetic[indexs[,ind]] - yhat4)^2)
}
c(mean(e1), mean(e2), mean(e3), mean(e4))
detach(ironslag)
```

The results show that the Quadratic model has the smallest leave-two-out error.   

## HW6         

### Question

- Ex1 
  Proof the Stationarity of Metropolis-Hastings sampler Algorithm in continuous situation.
- Exercises 8.1 8.3 (pages 242-243, Statistical Computing with R)
  
### Answers

#### Ex1

Let $f$ be the probability density function of the target distribution, and $Q$ be a transition distribution function, if $Q(x,|\cdot)$ has transition density $q(\cdot|x)$, then the acceptance probability is calculated after extracting a sample $y$ from $Q(x,|\cdot)$
$$
\alpha(x,y)=min\left\{1,\frac{f(y)q(x|y)}{f(x)q(y|x)}\right\}.
$$

For all $(x,y)$, such that $f(x)q(y|x)>0$, the resulting Markov chain transfer kernel is
$$p(x,y)=q(y|x)\alpha(x,y)=q(y|x)min\left\{1,\frac{f(y)q(x|y)}{f(x)q(y|x)}\right\}$$, so
$$
p(x,y)f(x)=f(x)q(y|x)\alpha(x,y)=p(y,x)f(y).
$$
Integrating both sides of the fine balance equation, we have
$$
\int p(x,y)f(x)dx=\int p(y,x)f(y)dx=f(y)
$$
By the definition of stationary distribution, we know that $f$ is stationary distribution.

#### Exercise 8.1

```{r,eval=TRUE}
library(stats)
library(datasets)
#Cramer-von Mises test
set.seed(1234)
two_sample_cvm_test <- function(X, Y,R=999) {
  # Calculate the empirical distribution function (ECDF) and sort the sample
  ecdf_X <- ecdf(X)
  ecdf_Y <- ecdf(Y)
  sorted_X <- sort(X)
  sorted_Y <- sort(Y)
  
  # Calculate the Cramer-von Mises statistic
  n <- length(X)
  m <- length(Y)
  CVM_statistic <- n*m/(m+n)^2*(sum((ecdf_X(sorted_X) - ecdf_Y(sorted_X))^2)+sum((ecdf_X(sorted_Y) - ecdf_Y(sorted_Y))^2))
    
  # calculate p-value
  permuted_CVM <- numeric(R) 
  combined_data <- c(X, Y)
  len<-m+n
  
  for (i in 1:R) {
    k <- sample(len,size=n,replace = FALSE)
    permuted_X <- combined_data[k]
    permuted_Y <- combined_data[-k]
    permuted_CVM[i] <- n*m/(m+n)^2*(sum((ecdf(permuted_X)(sort(permuted_X)) - ecdf(permuted_Y)(sort(permuted_X)))^2)+sum((ecdf(permuted_X)(sort(permuted_Y)) - ecdf(permuted_Y)(sort(permuted_Y)))^2))
  }
  
  p_value <- mean(c(CVM_statistic,permuted_CVM)>= CVM_statistic)
  
  # result
  return(round(p_value,4))
}

#load
attach(chickwts)
x <- sort(as.vector(weight[feed == "soybean"]))
y <- sort(as.vector(weight[feed == "linseed"]))
detach(chickwts)

# Cramer-von Mises test
result <- two_sample_cvm_test(x, y)

print(result)
```

#### Exercise 8.3

```{r,eval=TRUE}
library(stats)
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer(max(c(outx, outy)) > 5))
}

#generate data
set.seed(1234)
n1 <- 20
n2 <- 40
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
x <- x - mean(x) #centered by sample mean
y <- y - mean(y)

#permutation test
R <- 999 #number of replicates
z <- c(x, y) #pooled sample
len<-length(z)
tests <- numeric(R) #storage for replicates
test0 <- count5test(x, y)
for (i in 1:R) {
  #generate indices for the first sample
  k <- sample(len, size = length(x), replace = FALSE)
  x1 <- z[k]
  y1 <- z[-k] #complement of x1
  tests[i] <- count5test(x1,y1)
}
p <- mean(c(test0, tests))
round(p,4)
```

## HW7        

### Question

- Ex1(see figure in the pdf in qq)

- Ex2
  Exercises 9.4 9.7 9.10 (pages 277-278, Statistical Computing with R).

### Answers

#### Ex1

```{r,eval=TRUE}
library(stats)
library(graphics)
set.seed(1234)
#initialize
Na <- 1e6; b1a <- 0; b2a <- 1; b3a<--1;f0s <- c(0.1,0.01,0.001,0.0001)
as<-numeric(length(f0s))
finda<-function(N,b1,b2,b3,f0){
  x1<-rpois(N,1)
  x2<-rexp(N,1)
  x3<-rbinom(N,1,0.5)
  g <- function(alpha){
    tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3)
    p <- 1/(1+tmp)
    mean(p) - f0
  }
  solution<-uniroot(g,c(-20,0))
  a<-round(solution$root,5)
  return(a)
}
#show results
for(value in f0s){
  cat("f0=",value,"root is",finda(Na,b1a,b2a,b3a,value),"\n",sep="")
  as[which(f0s==value)]<-finda(Na,b1a,b2a,b3a,value)
}

#plots
plot(-log(f0s),as,main="The scatter plot of -log(f0) and a",xlab = "-log(f0)",ylab = "a")
plot(log(f0s),as,main="The scatter plot of log(f0) and a",xlab = "log(f0)",ylab = "a")
```

The relationship between log(f0) and a is linear.

#### Exercise 9.4      

standard Laplace function is $f(x)=\frac 1 2e^{-|x|}$. According to random walk M-H, the acceptance probability is
$$
r(X_t,Y)=\frac{f(Y)}{f(X_t)}=\frac{e^{-|y|}}{e^{-|x_t|}}
$$
where $Y=X_t+Z,Z\sim N(0,\sigma^2)$.    

```{r,eval=TRUE}
library(stats)
library(graphics)
set.seed(1234)
lap<-function(x){
  1/2*exp(-abs(x))
}
rw.Metropolis <- function(sigma, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (lap(y)/lap(x[i-1]))) x[i] <- y 
    else {
      x[i] <- x[i-1]
      k <- k + 1   #reject numbers
    }
  }
  return(list(x=x, k=k))
}

N <- 10000
sigma <- c(0.5, 1,2, 16)
x0 <- 25
rw1 <- rw.Metropolis(sigma[1], x0, N)
rw2 <- rw.Metropolis(sigma[2], x0, N)
rw3 <- rw.Metropolis(sigma[3], x0, N)
rw4 <- rw.Metropolis(sigma[4], x0, N)
#number of candidate points rejected
print(c(rw1$k, rw2$k, rw3$k, rw4$k))
#reject prob
print(c(rw1$k, rw2$k, rw3$k, rw4$k)/N)
#accept prob
print(c(1-rw1$k/N,1-rw2$k/N,1-rw3$k/N,1-rw4$k/N))
```

The second chain ($\sigma=1$) is more appropriate in terms of acceptance and rejection rates, with a rejection rate of about 0.3 and an acceptance rate of about 0.7, which can also be seen in the timing chart below.

```{r,eval=TRUE}
#plot ts
rw <- cbind(rw1$x, rw2$x, rw3$x, rw4$x)
for (j in 1:4) {
  plot(rw[,j], type="l",xlab=bquote(sigma == .(round(sigma[j],3))),ylab="X",ylim=range(rw[,j]))
}
```

```{r,eval=TRUE}
x <- seq(-6, 6, 0.01);burn<-2000
fx <- lap(x)
for (i in 1:4) {
  hist(rw[,i][-(1:burn)], breaks = "Scott", freq = FALSE, main = "",xlab = bquote(sigma == .(sigma[i])), xlim = c(-6, 6), ylim = c(0, 0.5),)
  lines(x, fx, col = 2, lty = 2)
}
```

#### Exercise 9.7

```{r,eval=TRUE}
library(stats)
set.seed(1234)
#initialize constants and parameters
N <- 5000 #length of chain
burn <- 1000 #burn-in length
X <- matrix(0, N, 2) #the chain, a bivariate sample
rho <- 0.9 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2
X[1, ] <- c(mu1, mu2) #initialize
for (i in 2:N) {
  x2 <- X[i-1, 2]
  m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
  X[i, 1] <- rnorm(1, m1, s1)
  x1 <- X[i, 1]
  m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
  X[i, 2] <- rnorm(1, m2, s2)
}
b <- burn + 1
x <- X[b:N, ]

#fit model
Xt<-x[,1];Yt<-x[,2]
lm.fit <- lm(Yt ~ Xt)
summary(lm.fit)
```

The regression coefficient obtained by fitting is 0.902807, which is very close to the correlation coefficient 0.9. Next, we test the normality and constant variance of the model residuals.
Based on the relationship between Y and X, theoretically there should be    
$$
\begin{aligned}
Var(e)&=Var(Y-0.9X)\\
&=Var(Y)+0.81Var(X)-1.8Cov(X,Y)\\
&=1+0.81-1.8*0.9*1=0.19  
\end{aligned}
$$

```{r,eval=TRUE}
library(graphics)
res <- lm.fit$residuals
#test normality
qqnorm(res)
qqline(res, col = "green", lwd = 2, lty = 2)

#test variance stability
qx <- seq(-3, 3, 0.01)
hist(res, breaks = "Scott", freq = FALSE, main = "", xlim = c(-3, 3), ylim = c(0, 1))
lines(qx, dnorm(qx, 0, sqrt(0.19)), col = 3, lwd = 1.5)
```

#### Exercise 9.10

```{r,eval=TRUE}
library("coda")
library(stats)
Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi) #row means
  B <- n * var(psi.means) #between variance est.
  psi.w <- apply(psi, 1, "var") #within variances
  W <- mean(psi.w) #within est.
  v.hat <- W*(n-1)/n + (B/n) #upper variance est.
  r.hat <- v.hat / W #G-R statistic
  return(r.hat)
}

```

```{r,eval=TRUE}
f <- function(x, sigma) {
  if (any(x < 0)) return (0)
  stopifnot(sigma > 0)
  return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
}
gen.chain<-function(sigma,N,x1){
  x<-numeric(N)
  x[1]<-x1
  u <- runif(N)
  for (i in 2:N) {
    xt <- x[i-1]
    y <- rchisq(1, df = xt)
    num <- f(y, sigma) * dchisq(xt, df = y)
    den <- f(xt, sigma) * dchisq(y, df = xt)
    if (u[i] <= num/den) x[i] <- y 
    else x[i] <- xt
  }
  return(x)
}
n <- 10000
sigma <- 4
k<-3  #number of chains
b <- 1000 #burn-in length

#choose overdispersed initial values
x0 <- c(rchisq(1, df=1),rchisq(1, df=2),rchisq(1, df=3))
#generate the chains
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
  X[i, ] <- gen.chain(sigma, n, x0[i])

#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))

#plot psi for the four chains
for (i in 1:k)
  plot(psi[i, (b+1):n], type="l",xlab=i, ylab=bquote(psi))

#plot the sequence of R-hat statistics
rhat <- rep(0, n)
for (j in (b+1):n)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.1, lty=2)
abline(h=1.2,lty=1)
```

From the results shown, the generated three chains can meet the requirement of $\hat R<1.2$. The following test is carried out with coda package, and the results are similar.

```{r,eval=TRUE}
l1<-as.mcmc(X[1,])
l2<-as.mcmc(X[2,])
l3<-as.mcmc(X[3,])
l<-mcmc.list(l1,l2,l3)
gelman.diag(l)
gelman.plot(l)
```

## HW8

### Question

- Ex1(see figure in the pdf in qq)

- Ex2
  Exercises 11.8 (pages 354, Statistical Computing with R).

### Answers

#### Ex1

(1) find MLE and prove consistency      
(1.1) observed MLE       
From the question, we have    
$$
\begin{aligned}
L_o\left(\lambda;u,v\right)&=\prod_{i=1}^{n}{P_\lambda\left(u_i\le x_i\le v_i\right)}=\prod_{i=1}^{n}{(e^{-\lambda u_i}-e^{-\lambda v_i})}\\
l_o\left(\lambda;u,v\right)&=\sum_{i=1}^{n}log\left(e^{-\lambda u_i}-e^{-\lambda v_i}\right)\\
\frac{\partial l_o\left(\lambda;u,v\right)}{\partial\lambda}&=\sum_{i=1}^{n}\frac{v_ie^{-\lambda v_i}-u_ie^{-\lambda u_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}
\end{aligned}
$$
Set the first derivative to 0 to get the residence point $\hat{\lambda}_{MLE,o}$, and then calculate the second derivative

\begin{equation*}\small
\begin{aligned}
\frac{\partial^2l_o\left(\lambda;u,v\right)}{\partial\lambda^2}&=\sum_{i=1}^{n} \frac{\left(-v_i^2e^{-\lambda v_i}+u_i^2e^{-\lambda u_i}\right)\left(e^{-\lambda u_i}-e^{-\lambda v_i}\right)-\left(v_ie^{-\lambda v_i}-u_ie^{-\lambda u_i}\right)\left(-u_i e^{-\lambda u_i}+v_i e^{-\lambda v_i}\right)}{\left(e^{-\lambda u_i}-e^{-\lambda v_i}\right)^2} \\
&=\sum_{i=1}^{n}\frac{-\left(u_i-v_i\right)^2e^{-\lambda\left(u_i+v_i\right)}}{\left(e^{-\lambda u_i}-e^{-\lambda v_i}\right)^2}<0
\end{aligned}
\end{equation*}

Thus $\hat{\lambda}_{MLE,o}$ is the extreme point, which is MLE.

(1.2) EM algorithm MLE     

\begin{equation*}\small
\begin{aligned}
L_c\left(\lambda;x\right)&=\prod_{i=1}^{n}{\lambda e^{-\lambda x_i}}=\lambda^ne^{-\lambda\sum_{i=1}^{n} x_i}\\
l_c\left(\lambda;x\right)&=nlog\lambda-\lambda\sum_{i=1}^{n}x_i\\
E_{\lambda^{(0)}}\left[l_c\left(\lambda;x\right)\middle|x_i\in\left[u_i,v_i\right]\right]&=E_{\lambda^{(0)}}\left[nlog\lambda-\lambda\sum_{i=1}^{n}x_i\middle|x_i\in\left[u_i,v_i\right]\right]    \\
&=nlog\lambda-\lambda E_{\lambda^{(0)}}\left[\sum_{i=1}^{n}x_i\middle|x_i\in\left[u_i,v_i\right]\right]
\end{aligned}
\end{equation*}
where $\lambda^{(0)}$ represents the initial value in each loop. Take the derivative of the left-hand side and set it to 0.   
$$
\begin{aligned}
\frac{n}{\lambda^{(k)}}&=E_{\lambda^{(k-1)}}\left[\sum_{i=1}^{n}x_i\middle|x_i\in\left[u_i,v_i\right]\right]\\
&=\left[\sum_{i=1}^{n}{\frac{1}{e^{-\lambda u_i}-e^{-\lambda v_i}}\int_{u_i}^{v_i}{\lambda x e^{-\lambda x}dx}}\right]_{\lambda=\lambda^{(k-1)}}\\
&=\left[-\sum_{i=1}^{n}\frac{v_ie^{-\lambda v_i}-u_ie^{-\lambda u_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}+\frac{1}{\lambda}\sum_{i=1}^{n}\frac{e^{-\lambda u_i}-e^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}\right]_{\lambda=\lambda^{(k-1)}}\\
&=-\frac{\partial l\left(\lambda;u,v\right)}{\partial\lambda}\bigg|_{\lambda=\lambda^{(k-1)}}+\frac{n}{\lambda^{(k-1)}}
\end{aligned}
$$

The iteration is then repeated until convergence, which is denoted as $\hat{\lambda}_{MLE,EM}$.

(1.3) Prove the consistency 

Rewrite the above equation as   
$$
\frac{1}{\lambda^{(k)}}=-\frac{1}{n}\frac{\partial l\left(\lambda;u,v\right)}{\partial\lambda}\bigg|_{\lambda=\lambda^{(k-1)}}+\frac{1}{\lambda^{(k-1)}}
$$

Define $y_k=\frac{1}{\lambda^{(k)}}$, then we have $y_k=y_{k-1}+\frac{1}{n}\sum_{i=1}^{n}\frac{u_ie^{-u_i/y_{k-1}}-v_ie^{-v_i/y_{k-1}}}{e^{- u_i/y_{k-1}}-e^{- v_i/y_{k-1}}}$, now we prove $\left\{y_k\right\}$ converges to the fixed point of $f(y)=y+\frac{1}{n}\sum_{i=1}^{n}\frac{u_ie^{-u_i/y}-v_ie^{-v_i/y}}{e^{- u_i/y}-e^{- v_i/y}}$, then the estimator by EM algorithm $\hat{\lambda}_{MLE,EM}$ converges to $\hat{\lambda}_{MLE,o}$ (It is then found that the first derivative of the observed likelihood is 0, i.e. the fixed point is $\hat{\lambda}_{MLE,o}$). Use the fixed point lemma, we only need to prove $|f^{\prime}(y)|\leq a<1$.

Define $g(y)=\frac{ue^{-u/y}-ve^{-v/y}}{e^{- u/y}-e^{- v/y}}$, we only need to prove $-1\leq g^{\prime}(y)<b(u,v)<0$. Take the derivation to get     
$$
g^{\prime}(y)=\frac{-\left(\frac{u-v}{y}\right)^2e^{-\frac{u+v}{y}}}{\left(e^{- u/y}-e^{- v/y}\right)^2}
$$    
Obviously we have $g^{\prime}(y)<0$, and due to when $y\in [y_1,y_2]$, $g^{\prime}(y)$ is bounded, take the upper bound for $i=1,\dots,n$ to get $g^{\prime}(y)<b(u,v)$ uniformly.

Another side is to prove $\left(\frac{u-v}{y}\right)^2e^{-\frac{u+v}{y}} \leq \left(e^{- u/y}-e^{- v/y}\right)^2$. Define $m=-u/y,n=-v/y$, we need to prove $(e^m-e^n)^2 \geq (m-n)^2e^{m+n}$, i.e. $e^{m-n}+e^{n-m}-2 \geq (m-n)^2$. Then we take $t=m-n$, and prove $e^t+e^{-t} \geq 2+t^2$, which is achieved by Taylor expansion $[1+t+t^2/2+t^3/3!+\cdots]+[1-t+t^2/2-t^3/3!+\cdots]=2+t^2+t^4/4!+\cdots \geq 2+t^2$.

In a word, we have $-1\leq g^{\prime}(y)<0$, which means $|f^{\prime}(y)|\leq a<1$, where $a$ is decided by $u_i,v_i,\lambda$. Then we have $y_k=\frac{1}{\lambda^{(k)}}\rightarrow \frac{1}{\lambda^{(\infty)}}=y_{\infty}$, and   
$$
\begin{aligned}
lim_{k \rightarrow \infty}\frac{|\lambda^{(k+1)}-\lambda^{(\infty)}|}{|\lambda^{(k)}-\lambda^{(\infty)}|}&=lim_{k \rightarrow \infty}\frac{|1/y_{k+1}-1/y_{\infty}|}{|1/y_{k}-1/y_{\infty}|}\\
&=lim_{k \rightarrow \infty}\frac{|y_k|}{|y_{k+1}|} \frac{|1-y_{k+1}/y_{\infty}|}{|1-y_{k}/y_{\infty}|}\\
&=\alpha \in (0,\infty)
\end{aligned}
$$
The convergence and linear convergence rate of EM algorithm are obtained.   

(2) Code 

```{r,eval=TRUE}
u<-c(11,8,27,13,16,0,23,10,24,2)
v<-c(12,9,28,14,17,1,24,11,25,3)
l1<-function(lambda){
  sum((v*exp(-lambda*v)-u*exp(-lambda*u))/(exp(-lambda*u)-exp(-lambda*v)))
}
#observe MLE
mle_o<-uniroot(l1,c(0,1),extendInt = "yes")$root

#EM
lambda0<-0.005
N<-1e5
tol<-1e-4
for(iter in 1:N){
  lambda1<-1/(1/lambda0-1/length(u)*l1(lambda0))
  if((abs(lambda1-lambda0)/lambda0)<=tol) break
  lambda0<-lambda1
}
mle_EM<-lambda1
mle_o
mle_EM
```

Here the result obtained by the EM algorithm has a certain relationship with the initial value selection, but the setting between 0.005-0.05 is close to the MLE obtained by the observation likelihood.

#### Exercise 11.8

```{r,eval=TRUE}
library(boot) #needed for simplex function
solve.game <- function(A) {
#solve the two player zero-sum game by simplex method
#optimize for player 1, then player 2
#maximize v subject to ...
#let x strategies 1:m, and put v as extra variable
#A1, the <= constraints
min.A <- min(A)
A <- A - min.A #so that v >= 0
max.A <- max(A)
A <- A / max(A)
m <- nrow(A)
n <- ncol(A)
it <- n^3
a <- c(rep(0, m), 1) #objective function
A1 <- -cbind(t(A), rep(-1, n)) #constraints <=
b1 <- rep(0, n)
A3 <- t(as.matrix(c(rep(1, m), 0))) #constraints sum(x)=1
b3 <- 1
sx <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=TRUE, n.iter=it)
#the â€™solutionâ€™ is [x1,x2,...,xm | value of game]
#
#minimize v subject to ...
#let y strategies 1:n, with v as extra variable
a <- c(rep(0, n), 1) #objective function
A1 <- cbind(A, rep(-1, m)) #constraints <=
b1 <- rep(0, m)
A3 <- t(as.matrix(c(rep(1, n), 0))) #constraints sum(y)=1
b3 <- 1
sy <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=FALSE, n.iter=it)
soln <- list("A" = A * max.A + min.A,
"x" = sx$soln[1:m],
"y" = sy$soln[1:n],
"v" = sx$soln[m+1] * max.A + min.A)
soln
}

```

```{r,eval=TRUE}
#enter the payoff matrix
A <- matrix(c( 0,-2,-2,3,0,0,4,0,0,
2,0,0,0,-3,-3,4,0,0,
2,0,0,3,0,0,0,-4,-4,
-3,0,-3,0,4,0,0,5,0,
0,3,0,-4,0,-4,0,5,0,
0,3,0,0,4,0,-5,0,-5,
-4,-4,0,0,0,5,0,0,6,
0,0,4,-5,-5,0,0,0,6,
0,0,4,0,0,5,-6,-6,0), 9, 9)
B<-A+2
sA <- solve.game(A)
sB<-solve.game(B)
round(cbind(sA$x, sA$y), 7)
round(cbind(sB$x, sB$y), 7)
```

The solution for both A and B is (11.15).

## HW9

### Question

- Ex1

  - 2.1.3 Exercise 4 (Pages 19 Advanced in R)

  - 2.3.1 Exercise 1, 2 (Pages 26 Advanced in R)

  - 2.4.5 Exercise 2, 3 (Pages 30 Advanced in R)

  - Exercises 2 (page 204, Advanced R)

  - Exercises 1 (page 213, Advanced R)

- Ex2    
  Consider Exercise 9.8 (pages 278, Statistical Computing with R). (Hint: Refer to the first example of Case studies section)
  
  - Write an R function.

  - Write an Rcpp function.

  - Compare the computation time of the two functions with the function "microbenchmark".
  
### Answers

#### 2.1.3 Ex4

_unlist_ can convert a list into an atomic vector, and can unpack the nested structure of a list into a narrow vector - an atomic vector. Atomic vectors contain only one type element and are often more efficient than lists when manipulating data.

as.vector converts a data structure into a vector, and a list is a generalized vector, so the structure is not changed, that is, it does not work. Here is an example.

```{r,eval=TRUE}
my_list <- list(1, 2, 3)
a<-as.vector(my_list)
b<-unlist(my_list)
typeof(a);typeof(b)
```

#### 2.3.1 Ex1,2  

```{r,eval=TRUE}
dim(b)
```
Return NULL.

If is.matrix(x) is TRUE, what will is.array(x) return?

```{r,eval=TRUE}
x<-matrix(1:4,nrow=2);is.matrix(x)
is.array(x)
```

Return TRUE.     

#### 2.4.5 EX2,3

What does as.matrix() do when applied to a data frame with columns of different types?

```{r,eval=TRUE}
df <- data.frame(x = 1:3, y = c("a", "b", "c"),stringsAsFactors = FALSE)
as.matrix(df)

df <- data.frame(x = 1:3, y = c(TRUE,FALSE,TRUE),stringsAsFactors = FALSE)
as.matrix(df)
```

When a non-numeric/logical variable type is returned, a character matrix is usually converted to the same type according to the cast rule, the conversion priority relationship is as follows (arrow indicates the conversion direction): [logical -> integer -> double -> character].


Can you have a data frame with 0 rows? What about 0 columns?

```{r,eval=TRUE}
df <- data.frame(a=matrix(nrow = 0,ncol=1),b=matrix(nrow = 0,ncol=1))
df
dim(df)

df <- data.frame(a=matrix(nrow = 2,ncol=0))
df
dim(df)
```

#### EX2 P204    

The function below scales a vector so it falls in the range [0,1]. How would you apply it to every column of a data frame?How would you apply it to every numeric column in a dataframe?

Standardize all columns:   
```{r,eval=TRUE}
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
df<-data.frame(x=c(1,2,3),y=c(4,4,5))
df
data.frame(lapply(df,scale01))
```

Standardize some columns:    

```{r,eval=TRUE}
df$z<-c("A","B","C")
df
spe_scale<-function(x){
  if (is.numeric(x)) scale01(x)
  else x
}
data.frame(lapply(df,spe_scale))
```

#### EX1 P213   

Use vapply() to:    
a) Compute the standard deviation of every column in a numeric data frame.       
b) Compute the standard deviation of every numeric column in a mixed data frame. (Hint: youâ€™ll need to use vapply() twice.)

```{r,eval=TRUE}
df<-data.frame(x=c(1,2,3),y=c(4,4,5))
df
vapply(df,sd,FUN.VALUE = 1)
```

```{r,eval=TRUE}
df<-data.frame(x=c(1,2,3),y=c(4,4,5))
df$z<-c("A","B","C")
df
vapply(df[vapply(df,is.numeric,TRUE)],sd,1)
```

#### Ex2

Gibbs sampling process:
First given y, extracting $x|y=y_{i-1} \sim bin(n,y)$, then extracting $y|x=x_{i} \sim beta(x+a,n-x+b)$.       

```{r,eval=TRUE}
library(stats)
library(finalwork)
library(microbenchmark)
library(graphics)
set.seed(1234)
gibbsR <- function(a,b,n,initial,N) {
  # a,b,n: fixed
  # initial: intial value
  # N: length of chain
  
  mat<-matrix(0,nrow = N, ncol = 2)
  mat[1,]<-initial
  for (i in 2:N) {
    y <- mat[i-1,2]
    mat[i,1]<-rbinom(1,n,y)
    x<-mat[i,1]
    mat[i,2]<-rbeta(1,(x+a),(n-x+b))
  }
  return (mat)
}
```

```{r,eval=TRUE}
#library(Rcpp)
# generate chains
N_0 <- 10000
burn <- 1000
a_0<-3;b_0<-4;n_0<-20
init<-c(sample(0:n_0,1),runif(1))
matR <- gibbsR(a_0,b_0,n_0,init,N_0)
XR <- matR[-(1:burn),1]; YR <- matR[-(1:burn),2]

#sourceCpp('gibbsC.cpp')
matC <- gibbsC(a_0,b_0,n_0,init,N_0)
XC <- matC[-(1:burn),1]; YC <- matC[-(1:burn),2]

qqplot(XR, XC, plot.it = TRUE)
abline(a = 0, b = 1, col = 2, lwd = 2, lty = 2)
qqplot(YR, YC, plot.it = TRUE)
abline(a = 0, b = 1, col = 2, lwd = 2, lty = 2)
```

It can be seen from the qq plots that the samples generated by the two methods are basically the same. Next, we compare the calculation time of the two functions.    

```{r,eval=TRUE}
ts <- microbenchmark(gibbsR = gibbsR(a_0,b_0,n_0,init,N_0),
                     gibbsC = gibbsC(a_0,b_0,n_0,init,N_0))
summary(ts)[, c(1, 3, 5, 6)]
```

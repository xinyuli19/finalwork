## ----eval=TRUE----------------------------------------------------------------
library(finalwork)
#loading 
data(HW0data1)
attach(HW0data1)
library(stats)
#label time series
xt <- ts(t(HW0data1), start = c(2000,1), frequency = 4) 
ts.plot(xt, main="The average income in USA", ylab="income")
#fit
y1 <- HoltWinters(xt)
plot(y1)
detach(HW0data1)

## ----eval=TRUE----------------------------------------------------------------
library(finalwork)
data(HW0data2)
attach(HW0data2)
library(ggplot2)
library(lubridate)
#plot
data_time <- data.frame(day = ymd(HW0data2[,1]), lowtemp =HW0data2[,3])
ggplot(data_time, aes(x=day, y=lowtemp))+geom_line(color="blue") + xlab("")+
scale_x_date(date_breaks = "2 years")+
labs(title="The lowest temperature in Hefei")+
theme(axis.text.x=element_text(angle=30,hjust=0.8,vjust=0.8))
detach(HW0data2)

## ----eval=TRUE----------------------------------------------------------------
library(graphics)
library(stats)
a<-1.4;b<--0.6
AR2 <- function(eps) {
  x=rep(0,length(eps))   #construct x
  x[1:2]=eps[1:2]        #initialize
  for (t in 3:length(eps)) {  #xt construct
    x[t]=a*x[t-1]+b*x[t-2]+eps[t]
  }
  return(x)
}

#set seed and generate series
set.seed(1234)
epsi <- rnorm(1000, mean = 0, sd=1)
Xt<- AR2(epsi)
plot(Xt, type="l",col="blue",xlab="time",ylab="",main=expression(X[t]))

## ----eval=TRUE----------------------------------------------------------------
my.sample<-function(x,size,replace=TRUE,prob=rep(1/length(x),length(x))){
  #size is the number of samples taken, and prob is the probability of sampling weight
  cp<-cumsum(prob)      #Divide the probability 1 interval from 0 to 1
  U<-runif(size)        
  r<-x[findInterval(U,cp)+1]    #Find the subscript of x corresponding to the interval of cp where u is, and then extract x
  return(r)      #Return the number of samples
}

s<-sample(c(1,2,4,10),size=10,replace=TRUE)
my.s<-my.sample(c(1,2,4,10),size=10,replace=TRUE)
print(s)
print(my.s)

## ----eval=TRUE----------------------------------------------------------------
library(graphics)
set.seed(2023)
n<-1000    #number of random variables
u<-runif(n)  #generate u
x<-numeric(length(u))
x[which(u<=1/2)]<-log(2*u[which(u<=1/2)])   #find u <=1/2
x[which(u>1/2)]<--log(2-2*u[which(u>1/2)]) 
#find u >1/2

#histogram plot
hist(x, prob = TRUE, main = expression(f(x)==1/2*e^{-x}),ylim=c(0,0.6))
y <- seq(-6, 6, 0.05)
lines(y, 1/2*exp(-abs(y)))

## ----eval=TRUE----------------------------------------------------------------
library(stats)
library(graphics)
set.seed(2023)
generate_beta<-function(n,a,b){
  #a,b~Beta(a,b)
  j<-k<-0;y<-numeric(n)
  x0<-(a-1)/(a+b-2)
  c<-x0^(a-1)*(1-x0)^(b-1)
  while(k<n){
    u<-runif(1)
    j<-j+1
    x<-runif(1)
    if(x^(a-1)*(1-x)^(b-1)/c>u){  #accept x
      k<-k+1;y[k]<-x
    }
  }
  return(y)
}

x<-generate_beta(1000,3,2)
#histogram plot
hist(x, prob = TRUE, main = paste('Beta(',3,',',2,')'))
y <- seq(0, 1, 0.01)
lines(y, y^(3-1)*(1-y)^(2-1)*gamma(3+2)/gamma(3)/gamma(2))

## ----eval=TRUE----------------------------------------------------------------
library(stats)
library(graphics)
set.seed(2023)
n<-10000
x<-numeric(n)
for (i in 1:n){
  u<-runif(3,-1,1)   #generate u1,u2,u3
  if (abs(u[3])>=abs(u[2]) & abs(u[3])>=abs(u[1])) x[i]<-u[2]
  else x[i]<-u[3]
}
#histogram plot
hist(x, prob = TRUE, main = expression(f(x)==3/4(1-x^2)))
y <- seq(-1, 1, 0.01)
lines(y, 3/4*(1-y^2))

## ----eval=TRUE----------------------------------------------------------------
library(stats)
set.seed(12345)
#set rho
rho1=0.3;rho2=0.7;rho3=1;d=1
#set n and K
n <- 1e6;K=100
pihat1<-numeric(K);pihat2<-numeric(K);pihat3<-numeric(K)
#MC simulation
for (k in 1:K){
  X <- runif(n,0,d/2)
  Y <- runif(n,0,pi/2)
  #calculate pihat
  pihat1[k]<-2*rho1/mean(rho1/2*sin(Y)>X)
  pihat2[k]<-2*rho2/mean(rho2/2*sin(Y)>X)
  pihat3[k]<-2*rho3/mean(rho3/2*sin(Y)>X)
}
pihat1_MC<-mean(pihat1)
pihat2_MC<-mean(pihat2)
pihat3_MC<-mean(pihat3)
var<-numeric(3)
#calculate var(pihat)
var[1]<-var(pihat1);var[2]<-var(pihat2);var[3]<-var(pihat3)
var

## ----eval=TRUE----------------------------------------------------------------
library(stats)
set.seed(12345)
m <- 1e4
x <- runif(m, min=0, max=1)
#MC
theta_hat1 <- mean(exp(x))
theta_hat2<-mean((exp(x)+exp(1-x))/2)
#theta_hat
theta_hat1
theta_hat2
#var(theta_hat)
var1 <- var(exp(x))                   
var2 <- var((exp(x)+exp(1-x))/2)      
#reduction ratio
100*(var1-var2)/var1

## ----eval=TRUE----------------------------------------------------------------
library(stats)
##define function
g<-function(x) 1/sqrt(2*pi)*x^2*exp(-x^2/2)*(x>1)
f1<-function(x) dnorm(x)
f2<-function(x) dgamma(x,3,1)

##sample
set.seed(123)
m <- 10000
theta.hat <- se<- numeric(2)
fg<-matrix(0,nrow=m,ncol=2)  #m*2, Column i stores the f/g value generated by fi

###use f1
x <- rnorm(m)
fg[,1]<-g(x)/f1(x)
theta.hat[1] <-mean(fg[,1])
se[1] <- sd(fg[,1])

###use f2
x <- rgamma(m,3,1)
fg[,2]<-g(x)/f2(x)
theta.hat[2] <-mean(fg[,2])
se[2] <- sd(fg[,2])

##show results
rbind(theta.hat, se)

## ----eval=TRUE----------------------------------------------------------------
summary(fg[,1])
summary(fg[,2])

## ----eval=TRUE----------------------------------------------------------------
g<-function(x) 1/sqrt(2*pi)*x^2*exp(-x^2/2)*(x>1)
f1<-function(x) dnorm(x)
f2<-function(x) dgamma(x,3,1)

##sample
set.seed(123)
m <- 10000
theta.hat<-numeric(m)
fg<-numeric(m)

###use f2
x <- rgamma(m,3,1)
fg<-g(x)/f2(x)
theta.hat <-mean(fg)

##MC estimate
theta.hat

## ----eval=TRUE----------------------------------------------------------------
library(stats)
m <- 10000
g <- function(x) exp(-x)/(1+x^2)*(x>0)*(x<1)
f <- function(x) exp(-x)/(1-exp(-1))*(x>0)*(x<1)

set.seed(123)
k<-5
n<-m/k  #Number of repetitions per interval
theta.hat <- var.hat <-numeric(k)
for(i in 1:k){
  u <- runif(n,(i-1)/5,i/5)
  x <- -log(1-(1-exp(-1))*u)  #generate x
  fg <- g(x)/k/f(x)   #gj/fj
  theta.hat[i]<-mean(fg)   #estimate
  var.hat[i]<-var(fg)
}
sum(theta.hat)
sqrt(sum(var.hat))

## ----eval=TRUE----------------------------------------------------------------
library(stats)
# set parameters
N <- 10000  # number of simulation
sample_size <- 20       
cl<- 0.95  # 1-alpha

# initialize
coverage <- numeric(N)

# MC
for (i in 1:N) {
  # generate samples
  sample_data <- rchisq(sample_size, df = 2)
  
  # calculate mean and sd
  sample_mean <- mean(sample_data)
  sample_se<-sd(sample_data) / sqrt(sample_size)
  
  # t-interval
  t_value <- qt((1 - cl) / 2, lower.tail=FALSE,df = sample_size - 1)
  lower_bound <- sample_mean - t_value * sample_se
  upper_bound <- sample_mean + t_value * sample_se
  
  # test
  true_mean_covered <- (lower_bound <= 2) & (upper_bound >= 2)
  
  # reserve results
  coverage[i] <- true_mean_covered
}

# prob
coverage_probability <- mean(coverage)
coverage_probability


## ----eval=TRUE----------------------------------------------------------------
library(stats)
set.seed(123)
##chi2
N <- 10000  
sample_size <- 30       
alpha<- 0.05  
mu0<-1
# initialize
p1 <- numeric(N)
p2 <- numeric(N)
for (j in 1:N) {
  x <- rchisq(sample_size, df = 1)
  ttest <- t.test(x,  mu = mu0)
  p1[j] <- ttest$p.value
  t_stat <- (mean(x) - mu0) / (sd(x) / sqrt(sample_size))
  p2[j]<-sum(abs(t_stat)>qt(1-alpha/2,df=sample_size-1))
}
p1.hat <- mean(p1 < alpha)
p2.hat<-mean(p2)
print(c(p1.hat, p2.hat))

## ----eval=TRUE----------------------------------------------------------------
library(stats)
set.seed(123)
##uniform
N <- 10000  
sample_size <- 30      
alpha<- 0.05 
mu0<-1
# initialize
p1 <- numeric(N)
p2 <- numeric(N)
for (j in 1:N) {
  x <- runif(sample_size, 0,2)
  ttest <- t.test(x,  mu = mu0)
  p1[j] <- ttest$p.value
  t_stat <- (mean(x) - mu0) / (sd(x) / sqrt(sample_size))
  p2[j]<-sum(abs(t_stat)>qt(1-alpha/2,df=sample_size-1))
}
p1.hat <- mean(p1 < alpha)
p2.hat<-mean(p2)
print(c(p1.hat, p2.hat))

## ----eval=TRUE----------------------------------------------------------------
library(stats)
set.seed(123)
##exp
N <- 10000  
sample_size <- 30       
alpha<- 0.05  
mu0<-1
# initialize
p1 <- numeric(N)
p2 <- numeric(N)
for (j in 1:N) {
  x <- rexp(sample_size, 1)
  ttest <- t.test(x,  mu = mu0)
  p1[j] <- ttest$p.value
  t_stat <- (mean(x) - mu0) / (sd(x) / sqrt(sample_size))
  p2[j]<-sum(abs(t_stat)>qt(1-alpha/2,df=sample_size-1))
}
p1.hat <- mean(p1 < alpha)
p2.hat<-mean(p2)
print(c(p1.hat, p2.hat))

## ----eval=TRUE----------------------------------------------------------------
library(stats)
set.seed(123)
m<-1000;M<-1000;alpha<-0.1  
reject1_h0<-matrix(0,nrow=M,ncol=m)
reject2_h0<-matrix(0,nrow=M,ncol=m)
result<-list()
result[[1]]<-matrix(0,nrow=M,ncol=3)  #Bonferroni
result[[2]]<-matrix(0,nrow=M,ncol=3)  #B-H
for (simu in 1:M){
  p_H0<-runif(m*0.95,0,1)
  p_Ha<-rbeta(m*0.05,0.1,1)
  p<-c(p_H0,p_Ha)
  p.adj1 <- p.adjust(p,method='bonferroni')
  p.adj2 <-p.adjust(p,method='fdr')
  
  #reject?
  reject1_h0[simu,] <- p.adj1 < alpha
  reject2_h0[simu,] <- p.adj2 < alpha
}

#FWER
fwer1<-0;fwer2<-0;fdr1<-0;fdr2<-0;tdr1<-0;tdr2<-0
for (simu in 1:M){
  fwer1<-fwer1+(sum(reject1_h0[simu,(1:m*0.95)])>0)
  fwer2<-fwer2+(sum(reject2_h0[simu,(1:m*0.95)])>0)
  fdr1<-fdr1+sum(reject1_h0[simu,(1:m*0.95)])/sum(reject1_h0[simu,])
  fdr2<-fdr2+sum(reject2_h0[simu,(1:m*0.95)])/sum(reject2_h0[simu,])
  tdr1<-tdr1+sum(reject1_h0[simu,(m*0.95+1):m])/(m*0.05)
  tdr2<-tdr2+sum(reject2_h0[simu,(m*0.95+1):m])/(m*0.05)
}

result<-matrix(0,nrow=2,ncol=3)
result[1,1]<-fwer1/M;result[2,1]<-fwer2/M 
result[1,2]<-fdr1/M;result[2,2]<-fdr2/M
result[1,3]<-tdr1/M;result[2,3]<-tdr2/M

## ----eval=TRUE----------------------------------------------------------------
result<-as.data.frame(result)
rownames(result)<-c("Bonferroni","B-H")
colnames(result)<-c("FWER","FDR","TPR")
knitr::kable(head(result),format = "markdown")

## ----eval=TRUE----------------------------------------------------------------
library(stats)
set.seed(123)
lambda_true<-2
sample_num<-c(5,10,20)
B<-1000
m<-1000  #simulation numbers
results<-list()
for(i in 1:length(sample_num)){  
  results[[i]]<-matrix(rep(0,m*2),nrow=m) 
  colnames(results[[i]])<-c("bias","sd")
}
for (sam in 1:length(sample_num)){   #1:3
  for (simu in 1:m){
    x<-rexp(sample_num[sam],lambda_true)  
    thetastar<- numeric(B)
    theta <- 1/mean(x)
    for(b in 1:B){
      xstar<-sample(x,replace = TRUE)
      thetastar[b]<-1/mean(xstar)
    }
    results[[sam]][simu,1]<-round(mean(thetastar)-theta,3)
    results[[sam]][simu,2]<-round(sd(thetastar),3)
  }
}
for (sam in 1:length(sample_num)){
  cat("sample number is ",sample_num[sam],",result is:","\n",sep="")
  result<-results[[sam]]
  cat("bootstrap bias mean",round(mean(result[,1]),3),"\n",sep="")
  cat("bootstrap sd mean",round(mean(result[,2]),3),"\n",sep="")
}

## ----eval=TRUE----------------------------------------------------------------
dat<-matrix(rep(0,3*4),nrow=3)
dat<-as.data.frame(dat)
rownames(dat)<-c("n=5","n=10","n=20")
colnames(dat)<-c("bias_bs","bias_theory","se_bs","se_theory")
for (sam in 1:length(sample_num)){
  result<-results[[sam]]
  dat[sam,c(1,3)]<-c(round(mean(result[,1]),3),round(mean(result[,2]),3))   #mean
  dat[sam,c(2,4)]<-c(round(lambda_true/(sample_num[sam]-1),3),round(lambda_true*sample_num[sam]/(sample_num[sam]-1)/sqrt(sample_num[sam]-2),3))   #theoretical value
  
}
knitr::kable(head(dat),format = "markdown")

## ----eval=TRUE----------------------------------------------------------------
#refer to the textbook
boot.t.ci <-function(x, B = 500, R = 100, level = .95, statistic){
#compute the bootstrap t CI
  x <- as.matrix(x); n <- nrow(x)
  stat <- numeric(B); se <- numeric(B)
  boot.se <- function(x, R, f) {  #x^(b) use bootstrap
    #local function to compute the bootstrap
    #estimate of standard error for statistic f(x)
    x <- as.matrix(x); m <- nrow(x)
    th <- replicate(R, expr = {
      i <- sample(1:m, size = m, replace = TRUE)
      f(x[i, ])})
    return(sd(th))
  }
  for (b in 1:B) { 
    j <- sample(1:n, size = n, replace = TRUE)
    y <- x[j, ]
    stat[b] <- statistic(y)
    se[b] <- boot.se(y, R = R, f = statistic)
  }
  stat0 <- statistic(x)
  t.stats <- (stat - stat0) / se
  se0 <- sd(stat)
  alpha <- 1 - level
  Qt <- quantile(t.stats, c(alpha/2, 1-alpha/2), type = 1)
  names(Qt) <- rev(names(Qt))
  CI <- rev(stat0 - Qt * se0)
}


## ----eval=TRUE----------------------------------------------------------------
library(bootstrap) #for the law data
data <- cbind(law$LSAT, law$GPA)  #data
stat <- function(data) {   
  cor(data[, 1],data[, 2])}
ci <- boot.t.ci(data, statistic = stat, B=2000, R=200)
print(ci)

## ----eval=TRUE----------------------------------------------------------------
library(boot) 
library(graphics)
set.seed(12345)
dat <- aircondit$hours #data
boot.mean <- function(x,i) mean(x[i])
de <- boot(data=dat,statistic=boot.mean, R = 2000)
ci <- boot.ci(de,type=c("norm","basic","perc","bca"))
print(ci)
hist(de$t, prob = TRUE,main="Bootstrap replicates")

## ----eval=TRUE----------------------------------------------------------------
library(bootstrap)
set.seed(12345)
dat <- scor;n<-nrow(dat)#data
#jackknife estimate bias
#theta_hat
cov.hat <- cov(scor)
evs.hat <- eigen(cov.hat)$values
theta.hat<-max(evs.hat)/sum(evs.hat)
#theta.jack
theta.jack <- numeric(n)
for(i in 1:n){
  cov.jack <- cov(scor[-i,])
  evs.jack <- eigen(cov.jack)$values
  theta.jack[i] <- max(evs.jack)/sum(evs.jack)
}
#bias&se
bias.jack <- (n-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((n-1)*mean((theta.jack-theta.hat)^2))
round(c(original=theta.hat,bias.jack=bias.jack,
se.jack=se.jack),3)

## ----eval=TRUE----------------------------------------------------------------
library(DAAG)
library(stats)
set.seed(12345)
attach(ironslag)
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n*(n-1)/2)
# for n-fold cross validation
# fit models on leave-two-out samples
indexs<-combn(1:n,2)
for (ind in 1:ncol(indexs)) {
  y <- magnetic[-indexs[,ind]]
  x <- chemical[-indexs[,ind]]
  
  J1 <- lm(y ~ x)
  yhat1 <- J1$coef[1] + J1$coef[2] * chemical[indexs[,ind]]
  e1[ind] <- sum((magnetic[indexs[,ind]] - yhat1)^2)
  
  J2 <- lm(y ~ x + I(x^2))
  yhat2 <- J2$coef[1] + J2$coef[2] * chemical[indexs[,ind]] +J2$coef[3] * chemical[indexs[,ind]]^2
  e2[ind] <- sum((magnetic[indexs[,ind]] - yhat2)^2)
  
  J3 <- lm(log(y) ~ x)
  logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[indexs[,ind]]
  yhat3 <- exp(logyhat3)
  e3[ind] <- sum((magnetic[indexs[,ind]] - yhat3)^2)
  
  J4 <- lm(log(y) ~ log(x))
  logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[indexs[,ind]])
  yhat4 <- exp(logyhat4)
  e4[ind] <- sum((magnetic[indexs[,ind]] - yhat4)^2)
}
c(mean(e1), mean(e2), mean(e3), mean(e4))
detach(ironslag)

## ----eval=TRUE----------------------------------------------------------------
library(stats)
library(datasets)
#Cramer-von Mises test
set.seed(1234)
two_sample_cvm_test <- function(X, Y,R=999) {
  # Calculate the empirical distribution function (ECDF) and sort the sample
  ecdf_X <- ecdf(X)
  ecdf_Y <- ecdf(Y)
  sorted_X <- sort(X)
  sorted_Y <- sort(Y)
  
  # Calculate the Cramer-von Mises statistic
  n <- length(X)
  m <- length(Y)
  CVM_statistic <- n*m/(m+n)^2*(sum((ecdf_X(sorted_X) - ecdf_Y(sorted_X))^2)+sum((ecdf_X(sorted_Y) - ecdf_Y(sorted_Y))^2))
    
  # calculate p-value
  permuted_CVM <- numeric(R) 
  combined_data <- c(X, Y)
  len<-m+n
  
  for (i in 1:R) {
    k <- sample(len,size=n,replace = FALSE)
    permuted_X <- combined_data[k]
    permuted_Y <- combined_data[-k]
    permuted_CVM[i] <- n*m/(m+n)^2*(sum((ecdf(permuted_X)(sort(permuted_X)) - ecdf(permuted_Y)(sort(permuted_X)))^2)+sum((ecdf(permuted_X)(sort(permuted_Y)) - ecdf(permuted_Y)(sort(permuted_Y)))^2))
  }
  
  p_value <- mean(c(CVM_statistic,permuted_CVM)>= CVM_statistic)
  
  # result
  return(round(p_value,4))
}

#load
attach(chickwts)
x <- sort(as.vector(weight[feed == "soybean"]))
y <- sort(as.vector(weight[feed == "linseed"]))
detach(chickwts)

# Cramer-von Mises test
result <- two_sample_cvm_test(x, y)

print(result)

## ----eval=TRUE----------------------------------------------------------------
library(stats)
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer(max(c(outx, outy)) > 5))
}

#generate data
set.seed(1234)
n1 <- 20
n2 <- 40
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
x <- x - mean(x) #centered by sample mean
y <- y - mean(y)

#permutation test
R <- 999 #number of replicates
z <- c(x, y) #pooled sample
len<-length(z)
tests <- numeric(R) #storage for replicates
test0 <- count5test(x, y)
for (i in 1:R) {
  #generate indices for the first sample
  k <- sample(len, size = length(x), replace = FALSE)
  x1 <- z[k]
  y1 <- z[-k] #complement of x1
  tests[i] <- count5test(x1,y1)
}
p <- mean(c(test0, tests))
round(p,4)

## ----eval=TRUE----------------------------------------------------------------
library(stats)
library(graphics)
set.seed(1234)
#initialize
Na <- 1e6; b1a <- 0; b2a <- 1; b3a<--1;f0s <- c(0.1,0.01,0.001,0.0001)
as<-numeric(length(f0s))
finda<-function(N,b1,b2,b3,f0){
  x1<-rpois(N,1)
  x2<-rexp(N,1)
  x3<-rbinom(N,1,0.5)
  g <- function(alpha){
    tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3)
    p <- 1/(1+tmp)
    mean(p) - f0
  }
  solution<-uniroot(g,c(-20,0))
  a<-round(solution$root,5)
  return(a)
}
#show results
for(value in f0s){
  cat("f0=",value,"root is",finda(Na,b1a,b2a,b3a,value),"\n",sep="")
  as[which(f0s==value)]<-finda(Na,b1a,b2a,b3a,value)
}

#plots
plot(-log(f0s),as,main="The scatter plot of -log(f0) and a",xlab = "-log(f0)",ylab = "a")
plot(log(f0s),as,main="The scatter plot of log(f0) and a",xlab = "log(f0)",ylab = "a")

## ----eval=TRUE----------------------------------------------------------------
library(stats)
library(graphics)
set.seed(1234)
lap<-function(x){
  1/2*exp(-abs(x))
}
rw.Metropolis <- function(sigma, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (lap(y)/lap(x[i-1]))) x[i] <- y 
    else {
      x[i] <- x[i-1]
      k <- k + 1   #reject numbers
    }
  }
  return(list(x=x, k=k))
}

N <- 10000
sigma <- c(0.5, 1,2, 16)
x0 <- 25
rw1 <- rw.Metropolis(sigma[1], x0, N)
rw2 <- rw.Metropolis(sigma[2], x0, N)
rw3 <- rw.Metropolis(sigma[3], x0, N)
rw4 <- rw.Metropolis(sigma[4], x0, N)
#number of candidate points rejected
print(c(rw1$k, rw2$k, rw3$k, rw4$k))
#reject prob
print(c(rw1$k, rw2$k, rw3$k, rw4$k)/N)
#accept prob
print(c(1-rw1$k/N,1-rw2$k/N,1-rw3$k/N,1-rw4$k/N))

## ----eval=TRUE----------------------------------------------------------------
#plot ts
rw <- cbind(rw1$x, rw2$x, rw3$x, rw4$x)
for (j in 1:4) {
  plot(rw[,j], type="l",xlab=bquote(sigma == .(round(sigma[j],3))),ylab="X",ylim=range(rw[,j]))
}

## ----eval=TRUE----------------------------------------------------------------
x <- seq(-6, 6, 0.01);burn<-2000
fx <- lap(x)
for (i in 1:4) {
  hist(rw[,i][-(1:burn)], breaks = "Scott", freq = FALSE, main = "",xlab = bquote(sigma == .(sigma[i])), xlim = c(-6, 6), ylim = c(0, 0.5),)
  lines(x, fx, col = 2, lty = 2)
}

## ----eval=TRUE----------------------------------------------------------------
library(stats)
set.seed(1234)
#initialize constants and parameters
N <- 5000 #length of chain
burn <- 1000 #burn-in length
X <- matrix(0, N, 2) #the chain, a bivariate sample
rho <- 0.9 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2
X[1, ] <- c(mu1, mu2) #initialize
for (i in 2:N) {
  x2 <- X[i-1, 2]
  m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
  X[i, 1] <- rnorm(1, m1, s1)
  x1 <- X[i, 1]
  m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
  X[i, 2] <- rnorm(1, m2, s2)
}
b <- burn + 1
x <- X[b:N, ]

#fit model
Xt<-x[,1];Yt<-x[,2]
lm.fit <- lm(Yt ~ Xt)
summary(lm.fit)

## ----eval=TRUE----------------------------------------------------------------
library(graphics)
res <- lm.fit$residuals
#test normality
qqnorm(res)
qqline(res, col = "green", lwd = 2, lty = 2)

#test variance stability
qx <- seq(-3, 3, 0.01)
hist(res, breaks = "Scott", freq = FALSE, main = "", xlim = c(-3, 3), ylim = c(0, 1))
lines(qx, dnorm(qx, 0, sqrt(0.19)), col = 3, lwd = 1.5)

## ----eval=TRUE----------------------------------------------------------------
library("coda")
library(stats)
Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi) #row means
  B <- n * var(psi.means) #between variance est.
  psi.w <- apply(psi, 1, "var") #within variances
  W <- mean(psi.w) #within est.
  v.hat <- W*(n-1)/n + (B/n) #upper variance est.
  r.hat <- v.hat / W #G-R statistic
  return(r.hat)
}


## ----eval=TRUE----------------------------------------------------------------
f <- function(x, sigma) {
  if (any(x < 0)) return (0)
  stopifnot(sigma > 0)
  return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
}
gen.chain<-function(sigma,N,x1){
  x<-numeric(N)
  x[1]<-x1
  u <- runif(N)
  for (i in 2:N) {
    xt <- x[i-1]
    y <- rchisq(1, df = xt)
    num <- f(y, sigma) * dchisq(xt, df = y)
    den <- f(xt, sigma) * dchisq(y, df = xt)
    if (u[i] <= num/den) x[i] <- y 
    else x[i] <- xt
  }
  return(x)
}
n <- 10000
sigma <- 4
k<-3  #number of chains
b <- 1000 #burn-in length

#choose overdispersed initial values
x0 <- c(rchisq(1, df=1),rchisq(1, df=2),rchisq(1, df=3))
#generate the chains
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
  X[i, ] <- gen.chain(sigma, n, x0[i])

#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman.Rubin(psi))

#plot psi for the four chains
for (i in 1:k)
  plot(psi[i, (b+1):n], type="l",xlab=i, ylab=bquote(psi))

#plot the sequence of R-hat statistics
rhat <- rep(0, n)
for (j in (b+1):n)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.1, lty=2)
abline(h=1.2,lty=1)

## ----eval=TRUE----------------------------------------------------------------
l1<-as.mcmc(X[1,])
l2<-as.mcmc(X[2,])
l3<-as.mcmc(X[3,])
l<-mcmc.list(l1,l2,l3)
gelman.diag(l)
gelman.plot(l)

## ----eval=TRUE----------------------------------------------------------------
u<-c(11,8,27,13,16,0,23,10,24,2)
v<-c(12,9,28,14,17,1,24,11,25,3)
l1<-function(lambda){
  sum((v*exp(-lambda*v)-u*exp(-lambda*u))/(exp(-lambda*u)-exp(-lambda*v)))
}
#observe MLE
mle_o<-uniroot(l1,c(0,1),extendInt = "yes")$root

#EM
lambda0<-0.005
N<-1e5
tol<-1e-4
for(iter in 1:N){
  lambda1<-1/(1/lambda0-1/length(u)*l1(lambda0))
  if((abs(lambda1-lambda0)/lambda0)<=tol) break
  lambda0<-lambda1
}
mle_EM<-lambda1
mle_o
mle_EM

## ----eval=TRUE----------------------------------------------------------------
library(boot) #needed for simplex function
solve.game <- function(A) {
#solve the two player zero-sum game by simplex method
#optimize for player 1, then player 2
#maximize v subject to ...
#let x strategies 1:m, and put v as extra variable
#A1, the <= constraints
min.A <- min(A)
A <- A - min.A #so that v >= 0
max.A <- max(A)
A <- A / max(A)
m <- nrow(A)
n <- ncol(A)
it <- n^3
a <- c(rep(0, m), 1) #objective function
A1 <- -cbind(t(A), rep(-1, n)) #constraints <=
b1 <- rep(0, n)
A3 <- t(as.matrix(c(rep(1, m), 0))) #constraints sum(x)=1
b3 <- 1
sx <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=TRUE, n.iter=it)
#the ’solution’ is [x1,x2,...,xm | value of game]
#
#minimize v subject to ...
#let y strategies 1:n, with v as extra variable
a <- c(rep(0, n), 1) #objective function
A1 <- cbind(A, rep(-1, m)) #constraints <=
b1 <- rep(0, m)
A3 <- t(as.matrix(c(rep(1, n), 0))) #constraints sum(y)=1
b3 <- 1
sy <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=FALSE, n.iter=it)
soln <- list("A" = A * max.A + min.A,
"x" = sx$soln[1:m],
"y" = sy$soln[1:n],
"v" = sx$soln[m+1] * max.A + min.A)
soln
}


## ----eval=TRUE----------------------------------------------------------------
#enter the payoff matrix
A <- matrix(c( 0,-2,-2,3,0,0,4,0,0,
2,0,0,0,-3,-3,4,0,0,
2,0,0,3,0,0,0,-4,-4,
-3,0,-3,0,4,0,0,5,0,
0,3,0,-4,0,-4,0,5,0,
0,3,0,0,4,0,-5,0,-5,
-4,-4,0,0,0,5,0,0,6,
0,0,4,-5,-5,0,0,0,6,
0,0,4,0,0,5,-6,-6,0), 9, 9)
B<-A+2
sA <- solve.game(A)
sB<-solve.game(B)
round(cbind(sA$x, sA$y), 7)
round(cbind(sB$x, sB$y), 7)

## ----eval=TRUE----------------------------------------------------------------
my_list <- list(1, 2, 3)
a<-as.vector(my_list)
b<-unlist(my_list)
typeof(a);typeof(b)

## ----eval=TRUE----------------------------------------------------------------
dim(b)

## ----eval=TRUE----------------------------------------------------------------
x<-matrix(1:4,nrow=2);is.matrix(x)
is.array(x)

## ----eval=TRUE----------------------------------------------------------------
df <- data.frame(x = 1:3, y = c("a", "b", "c"),stringsAsFactors = FALSE)
as.matrix(df)

df <- data.frame(x = 1:3, y = c(TRUE,FALSE,TRUE),stringsAsFactors = FALSE)
as.matrix(df)

## ----eval=TRUE----------------------------------------------------------------
df <- data.frame(a=matrix(nrow = 0,ncol=1),b=matrix(nrow = 0,ncol=1))
df
dim(df)

df <- data.frame(a=matrix(nrow = 2,ncol=0))
df
dim(df)

## ----eval=TRUE----------------------------------------------------------------
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
df<-data.frame(x=c(1,2,3),y=c(4,4,5))
df
data.frame(lapply(df,scale01))

## ----eval=TRUE----------------------------------------------------------------
df$z<-c("A","B","C")
df
spe_scale<-function(x){
  if (is.numeric(x)) scale01(x)
  else x
}
data.frame(lapply(df,spe_scale))

## ----eval=TRUE----------------------------------------------------------------
df<-data.frame(x=c(1,2,3),y=c(4,4,5))
df
vapply(df,sd,FUN.VALUE = 1)

## ----eval=TRUE----------------------------------------------------------------
df<-data.frame(x=c(1,2,3),y=c(4,4,5))
df$z<-c("A","B","C")
df
vapply(df[vapply(df,is.numeric,TRUE)],sd,1)

## ----eval=TRUE----------------------------------------------------------------
library(stats)
library(finalwork)
library(microbenchmark)
library(graphics)
set.seed(1234)
gibbsR <- function(a,b,n,initial,N) {
  # a,b,n: fixed
  # initial: intial value
  # N: length of chain
  
  mat<-matrix(0,nrow = N, ncol = 2)
  mat[1,]<-initial
  for (i in 2:N) {
    y <- mat[i-1,2]
    mat[i,1]<-rbinom(1,n,y)
    x<-mat[i,1]
    mat[i,2]<-rbeta(1,(x+a),(n-x+b))
  }
  return (mat)
}

## ----eval=TRUE----------------------------------------------------------------
#library(Rcpp)
# generate chains
N_0 <- 10000
burn <- 1000
a_0<-3;b_0<-4;n_0<-20
init<-c(sample(0:n_0,1),runif(1))
matR <- gibbsR(a_0,b_0,n_0,init,N_0)
XR <- matR[-(1:burn),1]; YR <- matR[-(1:burn),2]

#sourceCpp('gibbsC.cpp')
matC <- gibbsC(a_0,b_0,n_0,init,N_0)
XC <- matC[-(1:burn),1]; YC <- matC[-(1:burn),2]

qqplot(XR, XC, plot.it = TRUE)
abline(a = 0, b = 1, col = 2, lwd = 2, lty = 2)
qqplot(YR, YC, plot.it = TRUE)
abline(a = 0, b = 1, col = 2, lwd = 2, lty = 2)

## ----eval=TRUE----------------------------------------------------------------
ts <- microbenchmark(gibbsR = gibbsR(a_0,b_0,n_0,init,N_0),
                     gibbsC = gibbsC(a_0,b_0,n_0,init,N_0))
summary(ts)[, c(1, 3, 5, 6)]

